import json
from openai import OpenAI
import matplotlib.pyplot as plt
import os
from dotenv import load_dotenv

os.environ["OPENAI_API_KEY"]  = "sk-125fa70b20b84d0d9e3208604209c726"
os.environ["TOKENIZERS_PARALLELISM"] = "true"
load_dotenv('.env')


def read_json(file_path):
    with open(file_path, 'r') as file:
        data = json.load(file)
    return data

def evaluate_responses(original_question, model_a_response, model_b_response):
    prompt = f"""
I am comparing the responses generated by different models for a given question. I need you to evaluate these responses based on specific criteria and determine the winner for each criterion. Please avoid ties unless the responses are very similar.

Original Question: {original_question}

Graph RAG Response: {model_a_response}

Naive RAG Response: {model_b_response}

Evaluation Criteria:
1. Conciseness: The response that is more concise and to the point.
2. Information Coverage: The response that covers the most relevant information.
3. Relevance: The response that is most relevant to the original question.
4. Diversity: The response that provides a more diverse range of information or perspectives.

Please evaluate the responses based on the above criteria and provide the winner for each criterion in the following format. If there is a tie, indicate "Tie":

Conciseness: [Winner (Graph RAG, Naive RAG, or Tie)]
Information Coverage: [Winner (Graph RAG, Naive RAG, or Tie)]
Relevance: [Winner (Graph RAG, Naive RAG, or Tie)]
Diversity: [Winner (Graph RAG, Naive RAG, or Tie)]
"""


    openai_client = OpenAI(
    api_key=os.environ.get("OPENAI_API_KEY"), base_url="https://dashscope.aliyuncs.com/compatible-mode/v1", timeout=30
    )
    response = openai_client.chat.completions.create(
        model="qwen-max",  
        messages=[
            {"role": "system", "content": "You are an evaluator for comparing responses from different models."},
            {"role": "user", "content": prompt}
        ]
    )
    evaluation = response.choices[0].message.content.strip()
    return parse_evaluation(evaluation)


def parse_evaluation(evaluation_text):
    lines = evaluation_text.split('\n')
    result = {}
    for line in lines:
        if ':' in line:
            key, value = line.split(':', 1)
            result[key.strip()] = value.strip()
    return result


def calculate_accuracy(evaluations):
    criteria = ["Conciseness", "Information Coverage", "Relevance", "Diversity"]
    accuracy = {criterion: {"Graph RAG": 0, "Naive RAG": 0, "Tie": 0} for criterion in criteria}
    
    total_evaluations = len(evaluations)
    
    for eval in evaluations:
        for criterion, winner in eval.items():
            if criterion not in criteria: # Items of Explanations
                break
            if winner == "Graph RAG":
                accuracy[criterion]["Graph RAG"] += 1
            elif winner == "Naive RAG":
                accuracy[criterion]["Naive RAG"] += 1
            else:
                accuracy[criterion]["Tie"] += 1
    
    for criterion in criteria:
        for key in accuracy[criterion]:
            accuracy[criterion][key] /= total_evaluations
    
    return accuracy

def plot_accuracy(accuracy):
    criteria = list(accuracy.keys())
    labels = ["Graph RAG", "Naive RAG", "Tie"]
    colors = ['#D0DD97', '#E6B745', '#DDDDDD']

    fig, axs = plt.subplots(1, 4, figsize=(18, 4))
    fig.suptitle('Evaluation of different models')

    for i, criterion in enumerate(criteria):
        values = [accuracy[criterion][label] for label in labels]
        axs[i].bar(labels, values, color=colors)
        axs[i].set_xlabel('Models')
        axs[i].set_ylabel('Win Rate')
        axs[i].set_title(criterion)
        axs[i].set_ylim([0, 1])  

    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()


def main(global_file_path, naive_file_path, progress_callback=None):
    global_data = read_json(global_file_path)
    naive_data = read_json(naive_file_path)
    
    evaluations = []

    total_questions = sum(len(cases) for tasks in global_data.values() for cases in tasks.values())

    for user, tasks in global_data.items():
        for task, cases in tasks.items():
            for idx, entry in enumerate(cases):
                original_question = entry['q']
                model_a_response:str = entry['a']
                model_b_response:str = naive_data[user][task][idx]['a']
                
                if model_a_response.find("Sorry") != -1: # Cannot give answer
                    if progress_callback:
                        progress_callback(1)
                    continue
                
                evaluation = evaluate_responses(original_question, model_a_response, model_b_response)
                evaluations.append(evaluation)
                if progress_callback:
                    progress_callback(1)

    accuracy = calculate_accuracy(evaluations)
    return accuracy


if __name__ == "__main__":
    global_file_path = "result/global-answers.json"
    naive_file_path = "result/naive-answers.json"
    accuracy = main(global_file_path, naive_file_path)
    plot_accuracy(accuracy)    
    